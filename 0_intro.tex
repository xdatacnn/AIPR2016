\section{Introduction}
% no \IEEEPARstart
Recent estimates for daily data generation are around 2.5 quintillion ($10^{18}$) bytes. As an example, the instrument upgrades at the national laboratories, including accelerator and signal detector technology, are yielding an unprecedented increase in the quantity and complexity of output data from experimental facilities, such as synchrotrons and neutron sources~\cite{EOD:2015}. These instruments enable thousands of researchers to pursue investigations across a wide range of domains, from archeology and biology to physics and nano-science. While the science is diverse, the underlying image analyses required to measure fundamental quantities about the experiments follow a relatively smaller set of patterns. These patterns or motifs, when incorporated to emerging computer vision and machine learning (ML) algorithms, will ease scientists of the burden to leverage large repositories of curated data, and even enhance repositories by data augmentation for the discovery at scientifically relevant solution spaces.

Beyond conventional computer vision approaches~\cite{Ballard, Gonzalez, AFM:2015}, new schemes have emerged to address the semantic gap between signal acquisition and image interpretation~\cite{Zhang:2016:a}. In fact, convolutional neural networks turned into a mainstream ML algorithm that bridges massive image datasets to high-level visual description~\cite{Fei:2015}. Nonetheless, major challenges still remain unsolved in image understanding, such as those involving quantitative microscopy~\cite{Ushizima2016} of high-resolution data.

A word about kinds of CNN here and the role of these techniques as a data reduction mechanism; image signatures; much fewer bytes than the original image representation.

This paper will describe how to turn visual primitives, constrained to space and intensity variations, into higher-level semantics that can be used as computational motifs to understand image-centric data. We include the description and ..... \fixd{review here}
numerical schemes for automated characterization of materials components.

Our use-cases include records of high-resolution data, coming from scientific experiments, particularly those reliant on advanced instruments and simulations. We will report our preliminary results emphasizing three main tasks during the data analysis:
(i) Data sources: by coupling Observational, Simulation, and User-interaction data, how we can retrieve more relevant outcomes, particularly when using vast amounts of experimental records; (ii) Software tools: how we deploy ML algorithms using convolutional neural networks for conventional (Von Neumann) and new architectures, such as the low-power consumption IBM True
North neuromorphic chip; (iii) Use-cases: how the peculiarities of a science domain play a role in the algorithm development, how to extend algorithms learned in one science domain to other problems, which limitations to expect, and how we overcame some of the barriers imposed by the lack of curated data.



Use cases in Table~\ref{table1}: cryo-electron microscopy, X-ray diffraction, X-ray scattering and X-ray microtomography.

%From joao
The architecture of Convolutional Neural Networks (CNN) depend on a
large number of parameters, obtained from the data and derived weights,
found during a through search over the hyperparameter space. The
memory footprint to compute and store CNN models demand research
on methods to adapt CNN to energy efficient devices. This work focuses
on data reduction schemes and net weight representations in order to
accurately classify scientific data from simulations. Our scheme reduces
double-format values to one byte,maintaining classification accuracy
above 98\%.

-> aggregate

%This work incorporates data gathering, analytical schemes using CNN and exploratory visualization.
\input{table_modalities.tex}

\section{Related work}
Why neuromorphic:
Problem formulation: the most difficult part of computing, neuromorphic or not, is to figure out what to compute, e.g. scale;
Increase data volume, complexity (different imaging conditions, noise levels);
Optimize CNN architecture (number of layer, number of filters, filter size, downsample rate etc.);
Port to TrueNorth (Dharmendra Modhaâ€™s blog);
Comparing with other algorithms (e.g., SVM, k-means, kNN, bag of features/signatures):
Complexity and performance;
Flexibility;
Robustness in classification and searchable criterias.
